### Проект ETL-процесса формирования витрин данных для анализа публикаций новостей

##### Источники данных:
* https://lenta.ru/rss/
* https://www.vedomosti.ru/rss/news
* https://tass.ru/rss/v2.xml

В связи с тем, что в вышеуказанных источниках названия и разнообразие категорий новостей отличаются, требовалось их проанализировать и привести к единому виду. В результате была сформирована витрина данных, в которой содержатся:

* Суррогатный ключ категории;
* название категории;
* общее количество новостей из всех источников по данной категории за все время;
* количество новостей данной категории для каждого из источников за все время;
* общее количество новостей из всех источников по данной категории за последние сутки;
* количество новостей данной категории для каждого из источников за последние сутки;
* среднее количество публикаций по данной категории в сутки;
* день, в который было сделано максимальное количество публикаций по данной новости;
* количество публикаций новостей данной категории по дням недели.

В качестве базы данных был использован __PostgreSQL__. В ней хранятся данные из новостных rss-лент. Для обработки данных применялся язык __Python__. Оркестрация обработанных данных осуществлялась __Apache Airflow__. Для автоматизации создания, управления и развертывания приложения использовался __Docker__.

-----
#### Запуск
1. В корне проекта создать файл __.env__. В параметрах файла прописать креды Airflow и Postgres:
```
AIRFLOW_USER=airflow
AIRFLOW_PASSWORD=airflow
POSTGRES_HOST=host.docker.internal
POSTGRES_PORT=5432
POSTGRES_DB_NAME=etl_news
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
```
2. Проинициализировать Airflow командой __docker compose up airflow-init__
3. Запустить контейнеры командой __docker compose up -d__

_URL для входа в Airflow_ _http://localhost:8080_